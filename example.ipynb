{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbf29bae",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/YaoGroup/pinn_clusters/blob/main/example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f161be-8bbd-4075-9e33-ef4a4bce3017",
   "metadata": {
    "id": "64f161be-8bbd-4075-9e33-ef4a4bce3017",
    "tags": []
   },
   "source": [
    "# Example notebook\n",
    "\n",
    "\n",
    "#Setting up\n",
    "\n",
    "You can open this notebook open by clicking on the previous cell from github. You can also just clone this repo. If using Colab change your runtime to a t-4 instance and execute the following cell to install the software associated with the paper\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096d3e99-ce47-4c97-968a-f8f825a2b28d",
   "metadata": {
    "id": "096d3e99-ce47-4c97-968a-f8f825a2b28d"
   },
   "outputs": [],
   "source": [
    "!python -m pip install \" pinn_clusters @ git+https://github.com/YaoGroup/ice_1D_pinn.git@3239e08ce23fac93db566ee7bf5c1fea757806d9\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a807e721-dd71-4bb2-89bb-276f7a639648",
   "metadata": {},
   "source": [
    "The next cell only needs to be executed if you are using Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b9a56e-a829-43e1-a633-75d8cdc8745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/YaoGroup/pinn_clusters.git paper\n",
    "%cd paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f69ee5c-4d22-4e89-acb4-6baf6893df50",
   "metadata": {
    "id": "0f69ee5c-4d22-4e89-acb4-6baf6893df50"
   },
   "outputs": [],
   "source": [
    "\"\"\"Import external packages\"\"\"\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import math as m\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from formulations import gamma_batch\n",
    "\"\"\"Import internal packages\"\"\"\n",
    "#from formulations._formulations import (inverse_1st_order_equations, data_equations, get_collocation_points)\n",
    "#from formulations._loss import SquareLoss, SquareLossRandom\n",
    "#from formulations._formulations import get_collocation_points, to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45481dd-3c9c-45d4-9537-3e9652797998",
   "metadata": {
    "id": "0f69ee5c-4d22-4e89-acb4-6baf6893df50"
   },
   "outputs": [],
   "source": [
    "\"\"\"Import external packages\"\"\"\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import math as m\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from formulations import gamma_batch\n",
    "\"\"\"Import internal packages\"\"\"\n",
    "#from formulations._formulations import (inverse_1st_order_equations, data_equations, get_collocation_points)\n",
    "#from formulations._loss import SquareLoss, SquareLossRandom\n",
    "#from formulations._formulations import get_collocation_points, to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf850bf-5e84-4db8-9bcb-ff66f84f8f8a",
   "metadata": {
    "id": "cbf850bf-5e84-4db8-9bcb-ff66f84f8f8a",
    "outputId": "b02142eb-7d15-41a3-e490-93187fa3640a"
   },
   "outputs": [],
   "source": [
    "\"\"\"Define the domain of the problem\"\"\"\n",
    "#Import ground truth data for u,h and their x-positions (x) from which to build synthetic noisy training data\n",
    "data = loadmat('data/constantB_uh.mat') #file path to ground truth u(x), h(x) profiles. To test sinusoidal B(x) studied in our paper, replace with location of \"sinusoidalB_uh.mat\".\n",
    "x_star = np.transpose(data['x'])\n",
    "u_star = np.transpose(data['u'])[:, 0]\n",
    "h_star = np.transpose(data['h'])[:, 0]\n",
    "B_truth = np.ones_like(x_star) #B(x) profile used to solve for ground truth u and h profiles. REPLACE rhs with 0.5*np.cos(3*np.pi*x_star) + 1 to test the sinusoidal profile studied in our paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5517489a-1066-4144-82b1-0574e473bb0b",
   "metadata": {
    "id": "5517489a-1066-4144-82b1-0574e473bb0b"
   },
   "outputs": [],
   "source": [
    "\"\"\"Parameters\"\"\"\n",
    "# Data parameters\n",
    "N_t = 1001  # Number of collocation points\n",
    "N_ob = 401  # Number of training points.\n",
    "\n",
    "# Model parameters\n",
    "layers = [20,20,20,20,20,20, 3] #Number of hidden units in each layer.\n",
    "lyscl = [1, 1, 1, 1, 1, 1] #Standard deviation to set the scales for Xavier weight initialization.\n",
    "\n",
    "# Hyper parameters for the PINN\n",
    "fractional = False\n",
    "\n",
    "num_iterations_adam_resampled = 4000   #Number of iterations of Adam using collocation resampling\n",
    "num_iterations_adam_fixed = 2000      #Number of iterations of Adam with fixed collocation points\n",
    "num_iterations_lbfgs = 2000           #umber of iterations of LBFGS using fixed collocation points\n",
    "\n",
    "#helper function for storing the results from one or more values of gamma in one call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0560e2c8-0722-4a51-ba5c-9010fe8c73f1",
   "metadata": {
    "id": "0560e2c8-0722-4a51-ba5c-9010fe8c73f1"
   },
   "outputs": [],
   "source": [
    "def format_dict(dict_list):\n",
    "    berrs = []\n",
    "    uerrs = []\n",
    "    herrs = []\n",
    "    bpreds = []\n",
    "    d_losses = []\n",
    "    e_losses = []\n",
    "    t_losses = []\n",
    "    u_preds = []\n",
    "    h_preds = []\n",
    "    u_samp = []\n",
    "    h_samp = []\n",
    "\n",
    "    for i in range(len(dict_list)):\n",
    "        berrs.append(dict_list[i][\"B_err\"])\n",
    "        uerrs.append(dict_list[i][\"u_err\"])\n",
    "        herrs.append(dict_list[i][\"h_err\"])\n",
    "        bpreds.append(dict_list[i][\"B_p\"])\n",
    "        d_losses.append(dict_list[i][\"data_losses\"])\n",
    "        e_losses.append(dict_list[i][\"equation_losses\"])\n",
    "        t_losses.append(dict_list[i][\"total_losses\"])\n",
    "        u_preds.append(dict_list[i][\"u_p\"])\n",
    "        h_preds.append(dict_list[i][\"h_p\"])\n",
    "        u_samp.append(dict_list[i][\"u_sampled\"])\n",
    "        h_samp.append(dict_list[i][\"h_sampled\"])\n",
    "\n",
    "    new_dict = {\"berrs\" : np.asarray(berrs),\n",
    "                \"uerrs\" : np.asarray(uerrs),\n",
    "                \"herrs\" : np.asarray(herrs),\n",
    "                \"bpreds\" : np.asarray(bpreds),\n",
    "                \"d_losses\" : d_losses, #just keep as list\n",
    "                \"e_losses\" : e_losses, #just keep as list\n",
    "                \"t_losses\" : t_losses, #just keep as list\n",
    "                \"u_p\" : np.asarray(u_preds),\n",
    "                \"h_p\" : np.asarray(h_preds),\n",
    "                \"u_sampled\" : np.asarray(u_samp),\n",
    "                \"h_sampled\" : np.asarray(h_samp)\n",
    "               }\n",
    "    return new_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6d1aa0-7cfa-4ed3-ab5b-529e30a22dfb",
   "metadata": {
    "id": "de6d1aa0-7cfa-4ed3-ab5b-529e30a22dfb",
    "outputId": "e4424997-8457-4b3c-b86a-ac5b0154dd1b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# set the noise value of the training data\n",
    "test_noise = 0.3\n",
    "\n",
    "#select gammas to test:\n",
    "\n",
    "# some options\n",
    "### Option 1: choose gamma ratios logarithmically from 10^-4 to 10^8 ###\n",
    "# logratios = np.linspace(-4,8,13)\n",
    "# test_gammas = np.power(10,logratios)/(1+np.power(10,logratios))\n",
    "\n",
    "### Option 2: choose a single value of gamma (e.g. gamma = 0.5) ###\n",
    "gamma = 0.5\n",
    "test_gammas = np.asarray([gamma])\n",
    "\n",
    "#test gamma value(s) and store results in a python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63354ed-5840-4785-9f4c-87c1a68f493b",
   "metadata": {
    "id": "f63354ed-5840-4785-9f4c-87c1a68f493b",
    "outputId": "6210de5b-bc63-408e-a1e0-d4f6cf36a55a"
   },
   "outputs": [],
   "source": [
    "#test gamma value(s) and store results in a python dictionary\n",
    "results = gamma_batch(test_gammas, test_noise, x_star, u_star, h_star,\n",
    "                      layers,lyscl, N_ob, fractional, N_t,\n",
    "                      num_iterations_adam_resampled, num_iterations_adam_fixed,\n",
    "                      num_iterations_lbfgs, B_truth)\n",
    "result    = format_dict(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0d0c0b-a1cc-499b-9575-97d569b20d31",
   "metadata": {
    "id": "9c0d0c0b-a1cc-499b-9575-97d569b20d31"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793410cb-da21-4384-9d68-52bd4f4aa22c",
   "metadata": {
    "id": "793410cb-da21-4384-9d68-52bd4f4aa22c"
   },
   "outputs": [],
   "source": [
    "x_star = np.linspace(start = 0.0, stop = 1.0, num = 401) #define spatial domain\n",
    "\n",
    "#load the B, u, h profiles predicted by PINN\n",
    "\n",
    "u_prediction = result['u_p'][0]\n",
    "h_prediction = result['h_p'][0]\n",
    "B_prediction = result['bpreds'][0]\n",
    "\n",
    "#load the synthetic training data for u and h\n",
    "u_sampled = result['u_sampled'][0]\n",
    "h_sampled = result['h_sampled'][0]\n",
    "\n",
    "#load ground truth u, h, and B profiles (constant B(x))\n",
    "ground_B = np.ones_like(x_star)\n",
    "ground_u = loadmat('data/constantB_uh.mat')['u'].flatten()\n",
    "ground_h = loadmat('data/constantB_uh.mat')['h'].flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d343dc81-3c05-4cd5-91a8-7fa88e74228e",
   "metadata": {
    "id": "d343dc81-3c05-4cd5-91a8-7fa88e74228e"
   },
   "source": [
    "**1. Check the final $u(x)$, $h(x)$, and $B(x)$ profiles after training compared to the ground truth.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e3a6f0-0faf-4fae-8deb-da8e3604cf1f",
   "metadata": {
    "id": "89e3a6f0-0faf-4fae-8deb-da8e3604cf1f",
    "outputId": "ec03b340-4827-4028-c90b-83c531a3feaf"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols = 1, figsize = (8,6))\n",
    "\n",
    "ax.plot(x_star, u_prediction, label = 'Predicted $u(x)$')\n",
    "ax.plot(x_star, h_prediction, label = 'Predicted $h(x)$')\n",
    "ax.plot(x_star, B_prediction, label = 'Predicted $B(x)$')\n",
    "\n",
    "ax.plot(x_star, ground_u, linestyle = ':', color = 'black', label = 'Ground truth $u(x)$, $h(x)$, $B(x)$')\n",
    "ax.plot(x_star, ground_h, linestyle = ':', color = 'black')\n",
    "ax.plot(x_star, ground_B, linestyle = ':', color = 'black')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title('Ground Truth vs. Predicted Profiles', style = 'italic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63514a0-ae1d-4471-9fa1-12b53a637942",
   "metadata": {
    "id": "e63514a0-ae1d-4471-9fa1-12b53a637942"
   },
   "source": [
    "**2. Compare the ground truth $u(x)$ and $h(x)$ profiles to the synthetic training data generated and used in this trial.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b12cc2d-ce86-4faa-bc04-088dbca73710",
   "metadata": {
    "id": "7b12cc2d-ce86-4faa-bc04-088dbca73710"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols = 1, figsize = (8,6))\n",
    "ax.plot(x_star, u_sampled, 'o', markersize = 1.0, label = 'Training data, $u(x)$')\n",
    "ax.plot(x_star, h_sampled,'o', markersize = 1.0, label = 'Training data, $h(x)$')\n",
    "\n",
    "ax.plot(x_star, ground_u, color = 'blue', label = 'Ground truth, $u(x)$')\n",
    "ax.plot(x_star, ground_h, color = 'orange', label = 'Ground truth, $h(x)$')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title('Ground Truth vs. Training data', style = 'italic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d74f71-3360-45cb-8c5c-240c282c4093",
   "metadata": {
    "id": "17d74f71-3360-45cb-8c5c-240c282c4093"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb8129c-b41f-4078-b6a2-c9c2efeb6aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
